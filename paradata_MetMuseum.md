#The Metropolitan Museum - Paradata
I started off this project with the view that it would be a wonderful way to explore my own capabilities and technical prowess. This meant I needed a way to gather information from the Met Museum in a quick and accurate manner. I saw that the Met Museum provided an API (application programming interface) called [Scrapi.org](http://scrapi.org) that I could use in conjunction with R (programming language) and RStudio (integrated development environment). The first problem I faced was that Scrapi was down when I first started to work on the project. I notified the creator(s) of [MuseMeta](https://github.com/ropensci/musemeta), in this case [Scott Chamberlain](http://scottchamberlain.info/), and he confirmed that the Scrapi API was down, but that he’d notified the Met Museum in a hope to get it back online. 
I held out hope for a while I pursued other venues to prepare for the final information. I worked out what I was looking for and how I would get it. All of that information is in the Slideshow that this accompanies. I did all of my work on a desktop running Windows 7 Professional, though I did attempt it on a laptop running Ubuntu 14 and it does work. The desktop is significantly more powerful and is using a hardwired Ethernet connection for faster connections. My first step in the “other venues” research was to find out what happened if I had the data for only 30 objects. I used the other MuseMeta functions, which queried the Met Museum directly, and gathered 30 objects. I quickly saw the strange way that the Met gathered its information and standardized it (or rather, did not standardize it). After determining that the lack of standardization would be the biggest issue I would be faced with (if I could gather the data) I set about cleaning up the information as it was returned from R and MuseMeta. This wasn’t a particularly difficult task, but did require some understanding of regex to turn the raw data into a usable CSV (comma separated value) spreadsheet. 
After having cleaned it up I imported that CSV into Excel (though other spreadsheet software works too) I started by sorting columns and when data did not match what should have been in the column I inserted a blank space and filled it with “NULL-(whatever it was: period, date, culture, etc).” I know had a small amount of data that I could work with and use to test all other aspects of the project.
Over the next week I set about grabbing all of the source code for the 190 search pages that contained objects I cared about. I did this manually by navigating to the page and downloading the source code. If I had been thinking more clearly I would have used a tool, like wget, and used it to grab all of the source code for me. This would have been a significant time savings.
Since I now had all of the source code and I knew it contained the object identifiers (six digit codes) I needed to clean up all of the pages to leave me with just the identifiers. I did this using regex to separate the lines from all other lines in the source code, and then bookmarked the line I cared about and deleted all of the rest. I did this all manually per page. After I had done the initial clean I realised I could use a macro to do the work for me much faster than I could do it.
I finally had all 17,000-ish identifiers. When trying to run all of them through RStudio and Musemeta at once, it crashes. I had to separate the code into chunks (I chose 90 object identifiers at a time) and run them one after the other. I noticed that this causes, over time, the memory (RAM) to rise substantially and after about 40 pulls RStudio had to be restarted to begin running as quickly as before.
I had issues with certain pages that returned errors and I knew it would be one or two objects per page giving the error and not all of the objects, so I went back and started doing those pages again, but this time I separated the 90 identifier groups into two 45 identifier groups. This allowed me to find the objects that would fail.
I went back with all of the new data (over 17000 objects) and cleaned it up using the same methodology for cleaning the 30 objects. It worked and I had a full CSV with everything in it, including a lot of NULL.
I tried to import all of this into [Palladio](http://palladio.designhumanities.org/#/) and while the import worked the graphs were all unusable due to the incredible amount of data that was being thrown at it. I made some conclusions about donors and culture at this point and the continued working on the data itself. (This point is about 23 February 2015.)
My last steps, now that I had clean data, were to visualize it in ways that I hadn’t yet seen, and to come up with conclusions about what I had found. I found information about the credit lines and then I investigated who those people were. I found big names like J.P. Morgan Sr. and I found names that must be searched for in a variety of places, but some connections leaped out. J.P. Morgan and Edward C. Moore both connected through Tiffany & Co. What this means, I’m not really sure, but I speculate it has to do with culture and tastes of the time being dictated at a higher level than the middle or lower classes.
I made some visualizations and used them in presentations, but I quickly found out where I had been going wrong: I used a doughnut style pie-chart to represent all of the donors, but I had not normalized them against the total of objects and so it appeared that they had all donated the same amount of objects, just in different proportions to the object’s type.
Once I had properly decided how I was going to represent the data I had to figure out how I was going to display this data. I chose to use [Slides](http://slides.com/matthewdodd/the-metropolitan-museum#/) to display all of the information, though all of it is kept up on [GitHub](http://github.com/matthewdodd/hist4805b). I endeavored to have it all up in time so that it could be seen and commented upon.
Over all I did a lot of work that could have been shortened had some things fallen in to place, but because they didn’t I had to learn and explore in other directions. If I Could do this project over again, I would, but instead I’d do it with all of the objects and I’d import the CSV into a database for much faster manipulation of data. I learned a lot about the digital and modern side of ancient history and museums, as well as the (potentially) illicit side of antiquities. I’m not sure If I did this assignment the way that Dr. Graham had hoped I would or if I accomplished what I was expected to out of it all, but I think that I found out something (investigation), then I put it all together (combination), and then I showed everyone the work (demonstration/presentation/visualization).
